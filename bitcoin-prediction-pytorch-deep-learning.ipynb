{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bitcoin-prediction-pytorch-deep-learning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/sshillo/colab/blob/main/lstm_test_blog_bitcoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"nPhE8g9innF3"},"source":["First install Correct Libraries"]},{"cell_type":"code","metadata":{"id":"hzSCSH5ebm-G"},"source":["!pip install chart_studio plotly==4.9.0 statsmodels==0.11.0 pmdarima ipdb wandb pyarrow==2.0.0\n","!pip install pytorch-lightning==1.0.4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntrfcbTSnuYg"},"source":["Import everything we need\n"]},{"cell_type":"code","metadata":{"id":"vJ2wQO_tbOeZ"},"source":["import numpy as np\n","import pandas as pd\n","import plotly.graph_objects as go\n","import os\n","import torch.nn as nn\n","import torch\n","from torch.autograd import Variable\n","import ipdb\n","from torch.utils.data import TensorDataset, DataLoader, Dataset\n","device = torch.device(\"cuda:0\")\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","import shutil\n","from IPython.display import clear_output \n","import time\n","import urllib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OCnsi4Zqn83-"},"source":["I've separately collected 3 years bitcoin data at 15 minute intervals from the binance api. It is stored in parquet because it loads faster and preserves type information."]},{"cell_type":"code","metadata":{"id":"FBaBV1EfO17U"},"source":["bitcoin_data_url = \"https://drive.google.com/u/0/uc?id=14iEVdVtBaVfN6dMg0bO4QrfSaUoeXJ4Y&export=download\"\n","urllib.request.urlretrieve(bitcoin_data_url, \"data.parquet\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xm8Yn9OdoT1m"},"source":["Now lets create the Model. I'm using GRU instead of LSTM, because it's a little simpler in implementation"]},{"cell_type":"code","metadata":{"id":"e8zOffAPWBbp"},"source":["class GRU(nn.Module):\n","    def __init__(self, i_size, h_size, n_layers, o_size):\n","        super(GRU, self).__init__()\n","\n","        self.rnn = nn.GRU(\n","            input_size=i_size,\n","            hidden_size=h_size,\n","            num_layers=n_layers,\n","            batch_first=True\n","        )\n","        self.hidden_size = h_size\n","        self.num_layers = n_layers\n","        self.out = nn.Linear(h_size, o_size)\n","\n","    def init_hidden(self, batch_size=32):\n","      return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n","\n","    def forward(self, x, hidden=None):\n","        #num layers, batch size, hidden\n","        if hidden is None:\n","          batch_size = x.shape[0]\n","          hidden = self.init_hidden(batch_size)\n","          hidden = hidden.type_as(x)\n","\n","        out, next_hidden = self.rnn(x, hidden)\n","        outs = self.out(out[:,-1,:])\n","\n","        return outs, next_hidden \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfqDcui4oplg"},"source":["I'm going to create a function because functions are nice, they're reusable, if you want to transfer this code out of jupyter, it makes life a lot easier. Actually if I weren't writing a blog post, I would keep most of my code outside of the ipynb file, just importing the functions I need, and merely using the notebook as a way to display graphs inline. There's more to this tangent, but I'll leave that for another blog post."]},{"cell_type":"code","metadata":{"id":"_ynC2D16odW9"},"source":["def run(model_klass, \n","        dataset_train, \n","        dataset_test, \n","        sc, \n","        name='test',\n","        input_size =15,\n","        hidden_size=64,\n","        num_layers=2,\n","        output_size=1,\n","        num_epochs=3, \n","        batch_size=32,\n","        learning_rate=.001):\n","  print(f\"RUNNING {name}\")\n","  training_set = dataset_train\n","  training_set_scaled = sc.fit_transform(training_set)\n","\n","  X_train = []\n","  y_train = []\n","  for i in range(input_size, training_set_scaled.shape[0]):\n","      X_train.append(training_set_scaled[i-input_size:i, 0])\n","      y_train.append(training_set_scaled[i, 0])\n","  X_train, y_train = np.array(X_train), np.array(y_train)\n","\n","  X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n","\n","  rnn = model_klass(input_size, hidden_size, num_layers, output_size).to(device)\n","\n","  optimiser = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","  criterion = nn.MSELoss()\n","\n","  inputs = Variable(torch.from_numpy(X_train).float()).to(device)\n","  labels = Variable(torch.from_numpy(y_train).float()).to(device)\n","\n","  dataset = TensorDataset(inputs, labels)\n","  loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","  for epoch in range(num_epochs):\n","      losses = []\n","      hidden = rnn.init_hidden(batch_size=batch_size).to(device)\n","      for inputs, labels in loader:\n","        hidden = hidden.data\n","        output, hidden = rnn(inputs, None) \n","\n","        optimiser.zero_grad()\n","        loss = criterion(output.view(-1), labels)\n","        loss.backward()                     # back propagation\n","        optimiser.step()                                     # update the parameters\n","        losses.append(loss.item())\n","      if epoch % 5 == 0:\n","        print('epoch {}, loss {}'.format(epoch,np.mean(losses)))\n","\n","  real_stock_price = dataset_test #open values\n","  # Getting the predicted stock price of 2017\n","  dataset_total = np.concatenate((dataset_train, dataset_test), axis = 0)\n","  # inputs = dataset_total[len(dataset_total) - len(dataset_test) - INPUT_SIZE:].values\n","  inputs = dataset_total\n","  inputs = inputs.reshape(-1,1)\n","  # inputs = np.diff(inputs, axis=0)\n","  inputs = sc.transform(inputs)\n","  X_test = []\n","  for i in range(input_size, len(inputs)):\n","      X_test.append(inputs[i-input_size:i, 0])\n","  X_test = np.array(X_test)\n","  X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n","\n","  # X_train_X_test = np.concatenate((X_train, X_test),axis=0)\n","  test_inputs = Variable(torch.from_numpy(X_test).float()).to(device)\n","  # test_inputs = Variable(torch.from_numpy(X_train_X_test).float()).to(dev)\n","  predicted_stock_price, b = rnn(test_inputs)\n","  predicted_stock_price = np.reshape(predicted_stock_price.detach().cpu().numpy(), (test_inputs.cpu().shape[0], 1))\n","\n","  predicted_stock_price = sc.inverse_transform(predicted_stock_price)[:,0]\n","  # predicted_stock_price = diffinv(predicted_stock_price, start).reshape(-1, 1)\n","\n","  real_stock_price_all = dataset_total[input_size:][:,0]\n","\n","  # Visualising the results\n","  N = predicted_stock_price.shape[0]\n","  test_start = int(N * 0.75)\n","\n","  fig = go.Figure()\n","  fig.add_trace(go.Scatter(y=real_stock_price_all, name='Real'))\n","  fig.add_trace(go.Scatter(y=predicted_stock_price, name='Pred'))\n","  fig.add_shape(type=\"line\",\n","    x0=test_start, y0=0, x1=test_start, y1=20000,\n","    line=dict(color=\"RoyalBlue\",width=1))\n","  fig.add_trace(go.Scatter(\n","      x=[test_start - 5000], y=[15000],\n","      text=[\"Train/Test Split\"],\n","      mode=\"text\",\n","  ))\n","  fig.show()\n","\n","  # mean_squared_error(real_stock_price_all, predicted_stock_price, squared=False)\n","  t_d = predicted_stock_price[test_start:]\n","  r_d = real_stock_price_all[test_start:]\n","  test_rse = mean_squared_error(t_d, r_d, squared=False)\n","\n","  t_d = predicted_stock_price[0:test_start]\n","  r_d = real_stock_price_all[0:test_start]\n","  train_rse = mean_squared_error(t_d, r_d, squared=False)\n","  return name, train_rse, test_rse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPjeY666paw7"},"source":["Ok, now everything is setup, we're going to run everything"]},{"cell_type":"code","metadata":{"id":"QlYMXPINpZJk"},"source":["start = time.time()\n","\n","df = pd.read_parquet('./data.parquet')\n","cols = ['open']\n","data_train, data_test = train_test_split(df[cols].values, test_size=0.25, shuffle=False)\n","\n","errors = []\n","\n","sc = MinMaxScaler(feature_range = (-1, 1))\n","err = run(GRU, data_train, data_test, sc, 'gru minmax 0,1')\n","errors.append(err)\n","\n","total_time = time.time() - start\n","print(\"Total time:\", total_time)\n","\n","df = pd.DataFrame(errors)\n","df.columns = ['name','train error', 'test error']\n","print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JF0AX9YZpuft"},"source":["From the graph we can see that the model does a good, job predicting the price in just a few epochs."]}]}